# -*- coding: utf-8 -*-
"""Tom_and_Jerry_screentime_estimation_using_CNN_and_PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IQoDPGlZXruc0mu6cNg_zUz04yIjEANV
"""

!pip install split-folders

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import copy
import os
import torch
import shutil
import glob
from PIL import Image
from torch.utils.data import Dataset
import torchvision
import torchvision.transforms as transforms
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.nn as nn
from torchvision import utils
from torchvision.datasets import ImageFolder
from torchsummary import summary
import splitfolders
import torch.nn.functional as F
import pathlib
from torch import optim

from google.colab import drive

#Mount Google Drive
drive.mount('/content/drive')

folder_path = "/content/drive/MyDrive/tom_and_jerry"
files = os.listdir(folder_path)

# Display files
for file in files:
    print(file)

# Define the original dataset path
original_data_path = "/content/drive/MyDrive/tom_and_jerry"
backup_data_path = "/content/drive/MyDrive/original_tom_and_jerry"

# Copy dataset before splitting
if not os.path.exists(backup_data_path):
    shutil.copytree(original_data_path, backup_data_path)
    print("Backup dataset created successfully!")
else:
    print("Backup dataset already exists.")

data_dir='/content/drive/MyDrive/tom_and_jerry'
data_dir=pathlib.Path(data_dir)

splitfolders.ratio(data_dir, output="tom_and_jerry", seed=41, ratio=(.8,.2))

data_dir='/content/tom_and_jerry'
data_dir=pathlib.Path(data_dir)

data_dir

transform=transforms.Compose(
    [
        transforms.Resize((128,128)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(30),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]
)

train_set= torchvision.datasets.ImageFolder(data_dir.joinpath('train'),transform=transform)
train_set.transform
val_set= torchvision.datasets.ImageFolder(data_dir.joinpath('val'),transform=transform)
val_set.transform

print(train_set)

print(val_set)

img,label= train_set[100]
plt.imshow(img.permute(1,2,0))

batch_size=100
train_loader=torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=2,pin_memory=True)
val_loader=torch.utils.data.DataLoader(val_set,batch_size=batch_size,shuffle=True,num_workers=2,pin_memory=True)

next(iter(train_loader))[0].shape

next(iter(val_loader))[0].shape

class CNNModel(nn.Module):
  def _get_flattened_size(self):
      """Pass a dummy tensor through the conv layers to calculate the flattened size."""
      dummy_input = torch.randn(1, 3, 128, 128)  # (batch_size, channels, height, width)
      out = self.maxpool1(self.leaky_relu(self.conv1(dummy_input)))
      out = self.maxpool2(self.leaky_relu(self.conv2(out)))
      out = self.maxpool3(self.leaky_relu(self.conv3(out)))
      out = self.maxpool4(self.leaky_relu(self.conv4(out)))
      return out.view(1, -1).size(1)  # Compute flattened size dynamically

  def __init__(self):
    super(CNNModel,self).__init__()

    #Conv1
    self.conv1=nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1,padding=1)    #128-5+1=124

    #Maxpool 1
    self.maxpool1=nn.MaxPool2d(kernel_size=2)   #124/2 = 62

    #Conv2
    self.conv2=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5,stride=1,padding=1)   #62-5+1= 58

    #Maxpool 2
    self.maxpool2=nn.MaxPool2d(kernel_size=2)   #58/2 = 29

    #Conv3
    self.conv3=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5,stride=1,padding=1)   #29-5+1= 25

    #Maxpool 3
    self.maxpool3=nn.MaxPool2d(kernel_size=2)   #25/2 = 12

    #Conv4
    self.conv4=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=5,stride=1,padding=1)    #12-5+1= 8

    #Maxpool 4
    self.maxpool4=nn.MaxPool2d(kernel_size=2)   #8/2= 4

    #Activation Function
    self.leaky_relu=nn.LeakyReLU()

    #Automatically determine the flattened size
    self.flattened_size = self._get_flattened_size()

    #Fully connected alyer 1
    self.fc1=nn.Linear(self.flattened_size,1024)

    #Fully connected alyer 2
    self.fc2=nn.Linear(1024,4)



  def forward(self,x):
    #layer 1
    out=self.leaky_relu(self.conv1(x))
    out=self.maxpool1(out)

    #layer 2
    out=self.leaky_relu(self.conv2(out))
    out=self.maxpool2(out)

    #layer 3
    out=self.leaky_relu(self.conv3(out))
    out=self.maxpool3(out)

    #layer 4
    out=self.leaky_relu(self.conv4(out))
    out=self.maxpool4(out)

    #Flatten

    out=out.view(out.size(0), -1)

    #Linear Function

    out=self.leaky_relu(self.fc1(out))
    out=self.fc2(out)

    return out

#Training CNN
num_epochs=20
model=CNNModel()
error=nn.CrossEntropyLoss()
lr=0.001
optimizer=optim.Adam(model.parameters(),lr=lr)
scheduler=ReduceLROnPlateau(optimizer,mode='min',factor=0.5,patience=5)

train_loss_list=[]
val_loss_list=[]
accuracy_list=[]

for epoch in range(num_epochs):
  train_loss=0.0
  val_loss=0.0
  accuracy=0.0
  for i,(images,labels) in enumerate(train_loader):
    outputs=model(images)
    loss=error(outputs,labels)
    optimizer.zero_grad()
    train_loss+=loss.item()
    loss.backward()
    optimizer.step()
    print("loss in iteration: ",i,": ",loss.item())
  total=0
  correct=0
  for images,labels in val_loader:
    outputs=model(images)
    v_loss=error(outputs,labels)
    val_loss+=v_loss.item()
    predicted=torch.max(outputs.data,1)[1]

    total+=labels.size(0)
    correct+=(predicted==labels).sum()

  scheduler.step(val_loss/len(val_loader))
  accuracy=100*correct/float(total)
  train_loss_list.append(train_loss/len(train_loader))
  val_loss_list.append(val_loss/len(val_loader))
  accuracy_list.append(accuracy)

  print("Epoch:{} Loss:{} Accuracy:{}%".format(epoch+1,train_loss/len(train_loader),accuracy))

train_loss_list

val_loss_list

plt.plot(range(num_epochs),train_loss_list,label="Training Loss")
plt.plot(range(num_epochs),val_loss_list,label="Validation Loss")
plt.legend()
plt.show()

accuracy_list

plt.plot(range(num_epochs),accuracy_list,label="Accuracy")
plt.legend()
plt.show()

img,label=train_set[1000]
plt.imshow(img.permute(1,2,0))
print(label)
plt.title(label)

image=img.unsqueeze(0)

with torch.no_grad():
  output=model(image)
_,pred=torch.max(output,1)
print("Predicted output: ",pred.item())
# Check if prediction is correct
if label == pred.item():
    print("✅ Model predicted correctly!")
else:
    print("❌ Model predicted incorrectly.")

img,label=train_set[300]
plt.imshow(img.permute(1,2,0))
print(label)
plt.title(label)

image=img.unsqueeze(0)

with torch.no_grad():
  output=model(image)
_,pred=torch.max(output,1)
print("Predicted output: ",pred.item())
# Check if prediction is correct
if label == pred.item():
    print("✅ Model predicted correctly!")
else:
    print("❌ Model predicted incorrectly.")

img,label=val_set[1000]
plt.imshow(img.permute(1,2,0))
print(label)
plt.title(label)

image=img.unsqueeze(0)

with torch.no_grad():
  output=model(image)
_,pred=torch.max(output,1)
print("Predicted output: ",pred.item())
# Check if prediction is correct
if label == pred.item():
    print("✅ Model predicted correctly!")
else:
    print("❌ Model predicted incorrectly.")

img,label=val_set[500]
plt.imshow(img.permute(1,2,0))
print(label)
plt.title(label)

image=img.unsqueeze(0)

with torch.no_grad():
  output=model(image)
_,pred=torch.max(output,1)
print("Predicted output: ",pred.item())
# Check if prediction is correct
if label == pred.item():
    print("✅ Model predicted correctly!")
else:
    print("❌ Model predicted incorrectly.")

"""#Now we'll calculate screentime"""

# Path to the full dataset (original backup)
full_data_path = "/content/drive/MyDrive/original_tom_and_jerry"

"""*This will save your model's learned weights in Google Drive as tom_and_jerry_cnn.pth.*"""

# Save the trained model
torch.save(model.state_dict(), "/content/drive/MyDrive/tom_and_jerry_cnn.pth")
print("Model saved successfully!")

"""*Before running inference on the full dataset, load the model*"""

# Load the trained model
model = CNNModel()  # Initialize the CNN model
model.load_state_dict(torch.load("/content/drive/MyDrive/tom_and_jerry_cnn.pth", map_location=torch.device('cpu')))
model.eval()  # Set model to evaluation mode
print("Model loaded successfully!")

# Define transformation (same as training)
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

frame_paths = sorted(glob.glob(os.path.join(full_data_path, "**", "*.jpg"), recursive=True))

print(f"Total images found: {len(frame_paths)}")
print("Sample images:", frame_paths[:5])  # Print first 5 image paths

# Initialize class counts
class_counts = {0: 0, 1: 0, 2: 0, 3: 0}  # Assuming class 0=Jerry, 1=Tom, 2=Both, 3=Neither
fps = 1  # Change this if your video has a different FPS

# Classifies an image using the trained model.
def predict_image(image_path, model, transform):
    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0)  # Add batch dimension
    with torch.no_grad():
        output = model(image)
        _, predicted = torch.max(output, 1)
    return predicted.item()

# Iterate over all frames and classify them
for i, frame_path in enumerate(frame_paths):
    pred_class = predict_image(frame_path, model, transform)
    class_counts[pred_class] += 1
    if i % 500 == 0:  # Print progress every 500 frames
        print(f"Processed {i}/{len(frame_paths)} frames...")

# Convert frame counts to screen time (seconds)
screen_time = {k: v / fps for k, v in class_counts.items()}

# Print Results
print("\n--- Screen Time Results ---")
print(f"Total Frames Processed: {sum(class_counts.values())}")
print(f"Jerry Screen Time: {screen_time[0]:.2f} seconds")
print(f"Tom Screen Time: {screen_time[1]:.2f} seconds")
print(f"Both Characters: {screen_time[2]:.2f} seconds")
print(f"Neither Character: {screen_time[3]:.2f} seconds")

